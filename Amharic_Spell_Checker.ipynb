{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPkyU7jJOn7Ntl324MgfD6e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinsae17/tinsae17/blob/main/Amharic_Spell_Checker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ts5FmGCeb2M",
        "outputId": "28f41dcd-f961-43f1-be06-7c89ef10401d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "K1z7uNRx66D0"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import re\n",
        "from collections import Counter\n",
        "main_dir = '/content/drive/MyDrive/file/'\n",
        "raw_data_file_path =(main_dir + 'doc.txt')\n",
        "def words(document):\n",
        "    \"Convert text to lowercase and tokenize the document\"\n",
        "    return re.findall(r'\\w+', document.lower())\n",
        "\n",
        "# create a frequency table of all the words of the document\n",
        "all_words = Counter(words(open(main_dir + 'doc.txt').read()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "main_dir = '/content/drive/MyDrive/file/'\n",
        "# raw_data_file_path = main_dir + 'doc.txt'\n",
        "\n",
        "def words(document):\n",
        " \"Convert text to lower case and tokenize the document\"\n",
        " return re.findall(r'\\w+', document.lower())\n",
        "\n",
        "# create a frequency table of all the words of the document\n",
        "all_words = Counter(words(open(main_dir +'doc.txt').read()))\n",
        "\n",
        "def edits_one(word):\n",
        "    \"Create all edits that are one edit away from `word`.\"\n",
        "    alphabets = \"ሀሁሂሃሄህሆለሉሊላሌልሎሏሐሑሒሓሔሕሖሗመሙሚማሜምሞሟሠሡሢሣሤሥሦሧረሩሪራሬርሮሯሰሱሲሳሴስሶሷሸሹሺሻሼሽሾሿቀቁቂቃቄቅቆቇቈቊቋቌቍበቡቢባቤብቦቧቨቩቪቫቬቭቮቯተቱቲታቴትቶቷቸቹቺቻቼችቾቿኀኁኂኃኄኅኆኇኈኊኋኌኍነኑኒናኔንኖኗኘኙኚኛኜኝኞኟአኡኢኣኤእኦኧከኩኪካኬክኮኯኰኲኳኴኵኸኹኺኻኼኽኾወዉዊዋዌውዎዏዐዑዒዓዔዕዖዘዙዚዛዜዝዞዟዠዡዢዣዤዥዦዧየዩዪያዬይዮዯደዱዲዳዴድዶዷዸዹዺዻዼዽዾዿጀጁጂጃጄጅጆጇገጉጊጋጌግጎጏጐጒጓጔጕጠጡጢጣጤጥጦጧጨጩጪጫጬጭጮጯጰጱጲጳጴጵጶጷጸጹጺጻጼጽጾጿፀፁፂፃፄፅፆፇፈፉፊፋፌፍፎፏ\"\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes = [left + right[1:] for left, right in splits if right]\n",
        "    inserts = [left + c + right for left, right in splits for c in alphabets]\n",
        "    replaces = [left + c + right[1:] for left, right in splits if right for c in alphabets]\n",
        "    transposes = [left + right[1] + right[0] + right[2:] for left, right in splits if len(right)>1]\n",
        "    return set(deletes + inserts + replaces + transposes)\n",
        "\n",
        "def edits_two(word):\n",
        "    \"Create all edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits_one(word) for e2 in edits_one(e1))\n",
        "\n",
        "def known(words):\n",
        "    \"The subset of `words` that appear in the `all_words`.\"\n",
        "    return set(word for word in words if word in all_words)\n",
        "\n",
        "def possible_corrections(word):\n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits_one(word)) or known(edits_two(word)) or [word])\n",
        "\n",
        "def prob(word, N=sum(all_words.values())):\n",
        "    \"Probability of `word`: Number of appearances of 'word' / total number of tokens\"\n",
        "    return all_words[word] / N\n",
        "def stem_word(word):\n",
        "    suffixes = ['ም', 'ት', 'ህ', 'ዎች', 'ው']\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)]\n",
        "    return words\n",
        "def rectify(word):\n",
        "    \"return the most probable spelling correction for `word` out of all the `possible_corrections`\"\n",
        "    correct_word = max(possible_corrections(word), key=prob)\n",
        "    return correct_word"
      ],
      "metadata": {
        "id": "tQCe_mOtFs2Y"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_word =input(\"Enter an Amharic word or phrase to check: \")\n",
        "correction = \"Did you mean: \" + rectify(input_word)\n",
        "print(f\"Input: {input_word}\\nCorrection: {correction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9v1z5VXKI5P",
        "outputId": "36c274ba-d795-4a40-b04c-3fe8434730f2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter an Amharic word or phrase to check: መሆኑንመሆ\n",
            "Input: መሆኑንመሆ\n",
            "Correction: Did you mean: መሆኑን\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "SfiR5JNtT-QH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "yP-YgsDrUBi2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XE58nqL_WFBD",
        "outputId": "8e1e8a40-f730-4001-9e3c-631d5d158941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Requirement already satisfied: Levenshtein==0.25.1 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein) (0.25.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.25.1->python-Levenshtein) (3.9.3)\n",
            "Input: 'ምሳ'\n",
            "Correction: Did you mean: ምሳሌ, ነህ, ሰላም\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the file in read mode\n",
        "with open(main_dir +'doc.txt', 'r') as file:\n",
        "    # Read all lines into a list\n",
        "    lines = file.readlines()\n",
        "\n",
        "    # Check if there are at least 10 lines in the file\n",
        "    if len(lines) >= 10:\n",
        "        # Access and print the 10th line (index 9)\n",
        "        line_10 = lines[6]\n",
        "        print(line_10)\n",
        "    else:\n",
        "        print(\"The file does not have 10 lines.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G_PjtlRP6Nm",
        "outputId": "0d299a3f-cb10-4697-c347-bbba20df94b9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ነበር \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S-VLhsxTVXWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prob(word, N=sum(all_words.values())):\n",
        "    \"Probability of `word`: Number of appearances of 'word' / total number of tokens\"\n",
        "    return all_words[word] / N"
      ],
      "metadata": {
        "id": "qkxYHI_HES2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def known(words):\n",
        "    \"The subset of `words` that appear in the `all_words`.\"\n",
        "    return set(word for word in words if word in all_words)"
      ],
      "metadata": {
        "id": "aZRGWAAbD287"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def possible_corrections(word):\n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits_one(word)) or known(edits_two(word)) or [word])"
      ],
      "metadata": {
        "id": "SyrszuAVEBW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def edits_one(word):\n",
        "    \"Create all edits that are one edit away from `word`.\"\n",
        "    alphabets = \"ሀሁሂሃሄህሆለሉሊላሌልሎሏሐሑሒሓሔሕሖሗመሙሚማሜምሞሟሠሡሢሣሤሥሦሧረሩሪራሬርሮሯሰሱሲሳሴስሶሷሸሹሺሻሼሽሾሿቀቁቂቃቄቅቆቇቈቊቋቌቍበቡቢባቤብቦቧቨቩቪቫቬቭቮቯተቱቲታቴትቶቷቸቹቺቻቼችቾቿኀኁኂኃኄኅኆኇኈኊኋኌኍነኑኒናኔንኖኗኘኙኚኛኜኝኞኟአኡኢኣኤእኦኧከኩኪካኬክኮኯኰኲኳኴኵኸኹኺኻኼኽኾወዉዊዋዌውዎዏዐዑዒዓዔዕዖዘዙዚዛዜዝዞዟዠዡዢዣዤዥዦዧየዩዪያዬይዮዯደዱዲዳዴድዶዷዸዹዺዻዼዽዾዿጀጁጂጃጄጅጆጇገጉጊጋጌግጎጏጐጒጓጔጕጠጡጢጣጤጥጦጧጨጩጪጫጬጭጮጯጰጱጲጳጴጵጶጷጸጹጺጻጼጽጾጿፀፁፂፃፄፅፆፇፈፉፊፋፌፍፎፏ\"\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes = [left + right[1:] for left, right in splits if right]\n",
        "    inserts = [left + c + right for left, right in splits for c in alphabets]\n",
        "    replaces = [left + c + right[1:] for left, right in splits if right for c in alphabets]\n",
        "    transposes = [left + right[1] + right[0] + right[2:] for left, right in splits if len(right)>1]\n",
        "    return set(deletes + inserts + replaces + transposes)"
      ],
      "metadata": {
        "id": "qo-ehMbyNsY0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rectify(word):\n",
        "    \"Return the most probable spelling correction for `word` out of all the `possible_corrections`\"\n",
        "    correct_word = max(possible_corrections(word), key=prob)\n",
        "    return correct_word"
      ],
      "metadata": {
        "id": "TbxukKA1Ea-E"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}